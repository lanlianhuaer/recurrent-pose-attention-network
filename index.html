<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Recurrent Pose-Attention Network</title>
	<meta content="Wenbin, lanlianhuaer.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}

h1 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  # font-size: 16px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

img.pipeline {
  float: center;
  width: 770px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wanglimin.github.io');
  ga('send', 'pageview');

</script>
<body>

<div align = "center">
<h1>Video Action Recognition with Recurrent Pose-Attention Network</h1> <br />
<h3>Wenbin Du,Yali Wang, Yu Qiao</h3>
</div>

<div style="clear: both;">
<div class="paper">
  <img class="pipeline" src="img/FullPlotNew.png" title="actionness" />
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Abstract</h2>
  <div class="paper">
  The recent studies have demonstrated the effectiveness of Recurrent Neural Networks (RNNs) for action
recognition in videos. However, the previous works mainly
utilize video-level category as supervision to train RNNs,
which may prohibit RNNs to capture complex temporal structures along time. In this paper, we propose a Recurrent PoseAttention Network (RPAN) to address this challenge, where
we introduce a novel pose-attention mechanism to adaptively
learn pose-related features at every time-step action prediction of RNNs. First, unlike previous works on pose-related
action recognition, our RPAN is an end-to-end recurrent network, which can exploit the important spatial-temporal pose
evolutions to assist action recognition in a unified framework. Second, instead of using the human-joint features individually, our pose-attention can learn a number of robust
human-part features to alleviate the negative influence of
occlusions on action recognition. Furthermore, we flexibly
extend pose-attention to a RPAN ZERO version, in order
to handle the realistic scenarios where not all the joints are
shown in the videos. Third, one important byproduct of our
RPAN is video pose estimation, which can be used for coarse
annotations of human pose in videos. Finally, we evaluate
RPAN and its variants on the popular benchmarks of poserelated action recognition, i.e., Sub-JHMDB, PennAction and
JHMDB. The results show that, our RPAN outperforms the
state-of-the-art approaches. 
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Method</h2>
  <div class="paper">
 We proposed Recurrent Pose-Attention Network (RPAN), which can dynamically identify
the important pose-related feature to enhance every timestep action prediction of LSTM. 
	  <br>
	  First, the current frame is
fed into CNN to generate a convolutional feature cube. 
	  <br>
	  Then,
our pose-attention mechanism takes the previous hidden state
of LSTM as a guidance to estimate a number of human-partrelated features from the current convolutional cube. Due to
the fact that we partially share the attention parameters on
semantic-related human joints, the learnt human-part features can encode rich and robust body-structure-information
for occlusions. 
	  <br>
	  Finally, these features are fed into a humanpart pooling layer, which produces a discriminative 
	  poserelated feature for LSTM modeling.
	  <br><br>
		  
		  <br>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Motivations</h2>
  <div class="paper">
  <ul>
  <li><h3> Motivations</h3></li>
  <img src="img/ActionPosePlotnew.png"  width="770"/>
  
  </ul>
  </div>
</div>
</div>
	

<div style="clear: both;">
<div class="section">
  <h2>Results</h2>
  <div class="paper">
  <ul>
  <li><h3>Confusion Matrix Comparison（PennAction）</h3></li>
  <img src="img/ConfuseMatrix.png"  width="770"/>

  <li><h3>Feature Dynamics of Hidden State h in LSTM(Without Attention vs. Our Pose Attention)</h3> </li>
  <img src="img/Hactivated.png"  width="770"/>
  
  </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Downloads</h2>
  <div class="paper">
  <ul>
  <li><h3>The code of RPAN </h3> </li>
   Code on github [ <a href='https://github.com/lanlianhuaer/recurrent-pose-attention-network/'>Link</a> ]
   </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>References</h2>
  <div class="paper">
  A preliminary version of this work has been published in ICCV 2017, oral presentation <br> <br>
  Wenbin Du,Yali Wang, Yu Qiao<br>
   Rpan: An end-to-end recurrent pose-attention network for action recognition in videos <br>
  in IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2017
  </div>
</div>
</div>


</div>
</div>
<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 20th July, 2016</a></font></p>
</div>
</body>
</html>
