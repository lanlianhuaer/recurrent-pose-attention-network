<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Actionness Estimation</title>
	<meta content="Limin, wanglimin.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}

h1 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18pt;
  font-weight: 700;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  # font-size: 16px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

img.pipeline {
  float: center;
  width: 770px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wanglimin.github.io');
  ga('send', 'pageview');

</script>
<body>

<div align = "center">
<h1>Actionness Estimation Using Hybrid Fully Convolutional Networks</h1> <br />
<h3>Limin Wang, Yu Qiao, Xiaoou Tang, and Luc Van Gool</h3>
</div>

<div style="clear: both;">
<div class="paper">
  <img class="pipeline" src="img/actionness.png" title="actionness" />
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Abstract</h2>
  <div class="paper">
  The recent studies have demonstrated the effectiveness of Recurrent Neural Networks (RNNs) for action
recognition in videos. However, the previous works mainly
utilize video-level category as supervision to train RNNs,
which may prohibit RNNs to capture complex temporal structures along time. In this paper, we propose a Recurrent PoseAttention Network (RPAN) to address this challenge, where
we introduce a novel pose-attention mechanism to adaptively
learn pose-related features at every time-step action prediction of RNNs. First, unlike previous works on pose-related
action recognition, our RPAN is an end-to-end recurrent network, which can exploit the important spatial-temporal pose
evolutions to assist action recognition in a unified framework. Second, instead of using the human-joint features individually, our pose-attention can learn a number of robust
human-part features to alleviate the negative influence of
occlusions on action recognition. Furthermore, we flexibly
extend pose-attention to a RPAN ZERO version, in order
to handle the realistic scenarios where not all the joints are
shown in the videos. Third, one important byproduct of our
RPAN is video pose estimation, which can be used for coarse
annotations of human pose in videos. Finally, we evaluate
RPAN and its variants on the popular benchmarks of poserelated action recognition, i.e., Sub-JHMDB, PennAction and
JHMDB. The results show that, our RPAN outperforms the
state-of-the-art approaches. 
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Method</h2>
  <div class="paper">
  As shown in the above figure, we propose the architecture of hybrid fully convolutional networks (H-FCN) for actionness estimation in videos. In addition, based on actionness maps, we develop a action proposal generation method and present a more unified action detection framework. 
		  <br><br>
		  <li style = "list-style-type:square; margin-left:0px">
		  <strong>Hybrid fully convolutional networks: </strong> H-FCN is composed of appearance fully convolutional network (A-FCN) and motion fully convolutional network (M-FCN). These two FCNs capture visual information for actionness estimation from the perspectives of static appearance and dynamic motion, respectively.
		  </li>
		  <br>
		  <li style = "list-style-type:square; margin-left:0px">
		  <strong>Actionness estimation: </strong> A-FCN takes a single RGB image as input and M-FCN deals with two consecutive optical flow fields. In order to deal with scale variations,  We construct pyramid representations of RGB frames and stacking optical flow fields. These actionness maps from different scales are first up-sampled to the size of original image and then averaged.
		  </li>
		  <br>
		  <li style = "list-style-type:square; margin-left:0px">
		  <strong>Action proposal generation: </strong> Actionness maps are generic visual features and can be exploited for different vision problems. Here, we propose a sampling method to generate action proposals based on estimated actionness maps. We sample boxes according to their scores and spatial overlaps.
		  </li>
      <br>
      <li style = "list-style-type:square; margin-left:0px">
      <strong>Action detection: </strong> Following R-CNN, we train an action classifier with two-stream CNNs by cropping positive examples and mining negative examples. At test time, we directly use the output of two-stream CNNs as the detection score for each action proposal.
      </li>
		  <br>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Results</h2>
  <div class="paper">
  <ul>
  <li><h3>Examples of Actionness Maps and Action Proposals</h3></li>
  <img src="example.png"  width="770"/>

  <li><h3>Evaluation on Actionness Estimation (UCF Sports, Stanford40, and J-HMDB)</h3> </li>
  <div align=center>
  <img src="actionness1.png"  width="360" /> 
  <img src="actionness2.png"  width="350" />
  </div>

  <li><h3>Evaluation on Action Proposal (Stanford 40 and J-HMDB)</h3></li>
  <img src="proposal.png"  width="770"/>

  <li><h3>Evaluation on Action Detection (J-HMDB)</h3></li>
  <img src="detection.png"  width="770"/>
  
  </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Downloads</h2>
  <div class="paper">
  <ul>
  <li><h3>The code of TDD extraction:</h3> </li>
   Code on github [ <a href='https://github.com/wanglimin/actionness-estimation/'>Link</a> ]
   </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>References</h2>
  <div class="paper">
  If you use our trained model or code, please cite the following paper: <br> <br>
  Limin Wang, Yu Qiao, Xiaoou Tang, and Luc Van Gool <br>
  Actionness estimation using hybrid fully convolutional networks <br>
  in IEEE conference on computer vision and pattern recognition (<strong>CVPR</strong>), 2016
  </div>
</div>
</div>


</div>
</div>
<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 20th July, 2016</a></font></p>
</div>
</body>
</html>
